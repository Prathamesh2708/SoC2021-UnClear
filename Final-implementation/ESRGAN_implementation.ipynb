{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ESRGAN-implementation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZLpjzRS2OLK"
      },
      "source": [
        "# **ESRGAN implementation**\n",
        "-Final implementation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-MH2az12YLJ"
      },
      "source": [
        "**Importing necessary modules**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qa1IlEi6jXuN"
      },
      "source": [
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import time\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tensorflow.keras.layers import Add,Concatenate,LeakyReLU,Conv2D,Lambda,UpSampling2D\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ara7JFcZjFXe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNvtexQ7jODT"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/MyDrive/Kaggle\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a7BEOfU8yEW"
      },
      "source": [
        "tf.executing_eagerly()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0ueJcUSjcXl"
      },
      "source": [
        "\n",
        "%cd /content/gdrive/MyDrive/Kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Voro1boMjemT"
      },
      "source": [
        "!kaggle datasets download -d bansalyash/div2k-hr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zDGPNUpjjEn"
      },
      "source": [
        "!unzip \\*.zip  && rm *.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXJnOFHG2nHa"
      },
      "source": [
        "**Data preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YJ1yiej6Q0L"
      },
      "source": [
        "PATH = '/content/gdrive/MyDrive/Kaggle/DIV2K_train_HR'\n",
        "EPOCHS = 5\n",
        "\n",
        "def random_crop(input_image):\n",
        "    start_height = np.random.randint(0,input_image.shape[0]-96)\n",
        "    start_width = np.random.randint(0,input_image.shape[1]-96)\n",
        "    image = input_image[start_height:start_height+96 , start_width:start_width+96]\n",
        "\n",
        "    return image\n",
        "\n",
        "def load_hr(image_file):\n",
        "    image = tf.io.read_file(image_file)\n",
        "    image = tf.image.decode_png(image)\n",
        "    image = np.asarray(image)\n",
        "    hr_image = random_crop(image)\n",
        "\n",
        "    return hr_image\n",
        "\n",
        "def load_lr(hr_image):\n",
        "    lr_image = cv2.resize(hr_image, (24,24))\n",
        "\n",
        "    return lr_image\n",
        "\n",
        "def normalize(image):\n",
        "    image_t = tf.convert_to_tensor(image , dtype = tf.float32)\n",
        "    image_t = image_t/127.5 -1\n",
        "    return image_t\n",
        "\n",
        "\n",
        "train_dataset = os.listdir(PATH)\n",
        "for i in range(len(train_dataset)):\n",
        "  train_dataset[i] = PATH + '/'+train_dataset[i]\n",
        "train_hr_dataset = list(map(load_hr, train_dataset))\n",
        "train_lr_dataset = list(map(load_lr,train_hr_dataset))\n",
        "train_hr_dataset = tf.convert_to_tensor(list(map(normalize , train_hr_dataset)))\n",
        "train_lr_dataset = tf.convert_to_tensor(list(map(normalize , train_lr_dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKTNwKVuFwkC"
      },
      "source": [
        "#Just to check some of the photo processed\n",
        "for i in range(5):\n",
        "    plt.imshow(train_hr_dataset[i]/2 + 0.5)\n",
        "    plt.show()\n",
        "    plt.imshow(train_lr_dataset[i]/2 + 0.5)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA6hkDjUXh5T"
      },
      "source": [
        "**Building the Generator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQvsE934_1qt"
      },
      "source": [
        "def dense_block(inpt):\n",
        "    b1 = Conv2D(64, kernel_size=3, strides=1, padding='same')(inpt)\n",
        "    b1 = LeakyReLU(0.2)(b1)\n",
        "    b1 = Concatenate()([inpt,b1])\n",
        "    b2 = Conv2D(64, kernel_size=3, strides=1, padding='same')(b1)\n",
        "    b2 = LeakyReLU(0.2)(b2)\n",
        "    b2 = Concatenate()([inpt,b1,b2]) \n",
        "    b5 = Conv2D(64, kernel_size=3, strides=1, padding='same')(b2)\n",
        "    b5 = b5*0.2\n",
        "    b5 = Add()([b5, inpt])\n",
        "    return b5\n",
        "\n",
        "def RRDB(inpt):\n",
        "    x = dense_block(inpt)\n",
        "    x = dense_block(x)\n",
        "    x = x*0.2\n",
        "    out = Add()([x,inpt])\n",
        "\n",
        "    return out\n",
        "\n",
        "def buildGenerator():\n",
        "  inpt = tf.keras.Input(shape = [24,24,3])\n",
        "  up = UpSampling2D(4)(inpt)\n",
        "  conv1 =Conv2D(64 , kernel_size = 3 , strides = 1,padding = 'same')(up)\n",
        "  rrdb = RRDB(conv1)\n",
        "  conv = Conv2D(filters = 256, kernel_size = 3, strides = 1, padding = 'same',activation = 'relu')(rrdb)\n",
        "  out = Conv2D(3 , kernel_size= 3 , strides = 1 , padding = 'same')(conv)\n",
        "  return tf.keras.Model(inputs = inpt , outputs = out)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZH0oTpXhNk9"
      },
      "source": [
        "generator = buildGenerator()\n",
        "generator.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "626l7twu4B_d"
      },
      "source": [
        "**Building the Relativistic Discriminator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emLAxWpnL3kY"
      },
      "source": [
        "from keras import backend as K\n",
        "def build_discriminator():\n",
        "    \n",
        "    \n",
        "    leakyrelu_alpha = 0.2\n",
        "    momentum = 0.8\n",
        "\n",
        "    input_0 = tf.keras.layers.Input(shape=(24,24,3))\n",
        "    input_0_upscale = UpSampling2D(4)(input_0)\n",
        "\n",
        "    input_1 = tf.keras.layers.Input(shape=(96,96,3))\n",
        "    input_2 = tf.keras.layers.Input(shape = (96,96,3))\n",
        "\n",
        "    x = tf.keras.layers.concatenate([input_0_upscale,input_1])\n",
        "    y = tf.keras.layers.concatenate([input_0_upscale,input_2])\n",
        "    for i in range(4):\n",
        "      x = Conv2D(64 , kernel_size = 6 , strides = 1 , padding = 'same')(x)\n",
        "      y = Conv2D(64 , kernel_size = 6 , strides = 1 , padding = 'same')(y)\n",
        "      x = LeakyReLU()(x)\n",
        "      y = LeakyReLU()(y)\n",
        "      x = tf.keras.layers.BatchNormalization()(x)\n",
        "      y = tf.keras.layers.BatchNormalization()(y)\n",
        "\n",
        "    logits = x-K.mean(y)\n",
        "    # fully connected layer \n",
        "    output = Conv2D(1,4, activation='sigmoid' , padding = 'same')(logits)   \n",
        "    \n",
        "    model = tf.keras.Model(inputs=[input_0 , input_1,input_2], outputs=[output], name='discriminator')\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRXrWJNSbjbW",
        "outputId": "75a9a065-40d3-48ac-ba7c-bfde9dd8c0b6"
      },
      "source": [
        "discriminator = build_discriminator()\n",
        "discriminator.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 24, 24, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2D)  (None, 96, 96, 3)    0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            [(None, 96, 96, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 96, 96, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 96, 96, 6)    0           up_sampling2d_1[0][0]            \n",
            "                                                                 input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 96, 96, 6)    0           up_sampling2d_1[0][0]            \n",
            "                                                                 input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 96, 96, 64)   13888       concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 96, 96, 64)   13888       concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)       (None, 96, 96, 64)   0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 96, 96, 64)   0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 96, 96, 64)   256         leaky_re_lu_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 96, 96, 64)   256         leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 96, 96, 64)   147520      batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 96, 96, 64)   147520      batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)       (None, 96, 96, 64)   0           conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)       (None, 96, 96, 64)   0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 96, 96, 64)   256         leaky_re_lu_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 96, 96, 64)   256         leaky_re_lu_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 96, 96, 64)   147520      batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 96, 96, 64)   147520      batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)       (None, 96, 96, 64)   0           conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)       (None, 96, 96, 64)   0           conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 96, 96, 64)   256         leaky_re_lu_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 96, 96, 64)   256         leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 96, 96, 64)   147520      batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 96, 96, 64)   147520      batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)      (None, 96, 96, 64)   0           conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)      (None, 96, 96, 64)   0           conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 96, 96, 64)   256         leaky_re_lu_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 96, 96, 64)   256         leaky_re_lu_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.reduce_mean (TFOpLambda ()                   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.subtract (TFOpLambda)   (None, 96, 96, 64)   0           batch_normalization_6[0][0]      \n",
            "                                                                 tf.math.reduce_mean[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 96, 96, 1)    1025        tf.math.subtract[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 915,969\n",
            "Trainable params: 914,945\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-sjZvB834It"
      },
      "source": [
        "**Building the ESRGAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLl4fzdTbmr0"
      },
      "source": [
        "def relativistic_loss(disc_real,disc_gen):\n",
        "    real = disc_real\n",
        "    fake = disc_gen\n",
        "    fake_logits = K.sigmoid(fake - K.mean(real))\n",
        "    real_logits = K.sigmoid(real - K.mean(fake))\n",
        "            \n",
        "    return [fake_logits, real_logits]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhVHZ9LjcY5l"
      },
      "source": [
        "\n",
        "def discriminator_loss(fake_logits , real_logits) :\n",
        "  return  K.mean(K.binary_crossentropy(K.zeros_like(fake_logits),fake_logits)+K.binary_crossentropy(K.ones_like(real_logits),real_logits))\n",
        "\n",
        "def generator_loss(fake_logits , real_logits) :\n",
        "  return  K.mean(K.binary_crossentropy(K.zeros_like(real_logits),real_logits)+K.binary_crossentropy(K.ones_like(fake_logits),fake_logits))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mBeemIMxRdv"
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "\n",
        "\n",
        "# The VGG model for VGG loss , made for our input shape \n",
        "vgg = VGG19(include_top = False, input_shape=(96,96,3))\n",
        "\n",
        "Lambda = 0.05\n",
        "Eeta = 1 #both these values are supposed to be changed after epochs. The initial values are such that the GAN first predicts a rough figure about the images\n",
        "EPOCHS = 50\n",
        "\n",
        "\n",
        "def train_step(input_lr_image, target, epoch):\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        gen_output = generator(input_lr_image, training=True)\n",
        "\n",
        "        real_logits  = discriminator([input_lr_image, target , gen_output], training=True)\n",
        "        fake_logits  = discriminator([input_lr_image, target , gen_output], training=True)\n",
        "        gen_loss     = Lambda*generator_loss(fake_logits, real_logits)\n",
        "        gen_loss    += Eeta*tf.reduce_mean(tf.abs(target - gen_output))\n",
        "        feature_gen  = vgg(preprocess_input(gen_output))\n",
        "        feature_real = vgg(preprocess_input(np.copy(target)))\n",
        "        vgg_loss     = tf.keras.losses.mean_squared_error(feature_gen , feature_real)\n",
        "        gen_loss    += 100*vgg_loss\n",
        "        disc_loss = discriminator_loss(fake_logits, real_logits)\n",
        "\n",
        "    generator_gradients = gen_tape.gradient(gen_loss,\n",
        "                                          generator.trainable_variables)\n",
        "    discriminator_gradients = disc_tape.gradient(disc_loss,\n",
        "                                               discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(generator_gradients,\n",
        "                                          generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
        "                                              discriminator.trainable_variables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0F8SKiyY3vnk"
      },
      "source": [
        "def fit(train_lr,train_hr, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        start = time.time()\n",
        "        print(\".\")\n",
        "        for i in range(150):\n",
        "          input_image  = train_lr[4*i:4*i+4]\n",
        "          target  =  train_hr[4*i:4*i+4]\n",
        "          train_step(input_image,target , epoch)\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "        generated = generator(train_lr_dataset[0:5])\n",
        "        for i in range(len(generated)):\n",
        "          plt.subplot(1,3,1)\n",
        "          plt.imshow(train_lr_dataset[i]/2+0.5)\n",
        "          plt.subplot(1,3,2)\n",
        "          plt.imshow(train_hr_dataset[i]/2+0.5)\n",
        "          plt.subplot(1,3,3)\n",
        "          plt.imshow(generated[i]/2 + 0.5)\n",
        "          plt.show()\n",
        "\n",
        "\n",
        "        print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
        "                                                        time.time()-start))\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "checkpoint_dir = './ESRGAN_checkpoints/'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,discriminator_optimizer=discriminator_optimizer,generator=generator,discriminator=discriminator)\n",
        "\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "fit(train_lr_dataset,train_hr_dataset , EPOCHS)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qKDtfl26r9M"
      },
      "source": [
        "\n",
        "import random\n",
        "for i in range(70,80):\n",
        "  generated = generator(train_lr_dataset[10*i:10*i+10])\n",
        "  for j in range(9):\n",
        "      plt.subplot(1,3,1)\n",
        "      plt.imshow(train_lr_dataset[j+10*i]/2+0.5)\n",
        "      plt.subplot(1,3,2)\n",
        "      plt.imshow(train_hr_dataset[j+10*i]/2+0.5)\n",
        "      plt.subplot(1,3,3)\n",
        "      plt.imshow(generated[j]/2 + 0.5)\n",
        "      plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KyU5c4M-p6ex",
        "outputId": "21d0a345-b870-477c-e360-3a3ef0bcb73f"
      },
      "source": [
        "checkpoint.save(file_prefix = checkpoint_prefix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./ESRGAN_checkpoints/ckpt-4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    }
  ]
}